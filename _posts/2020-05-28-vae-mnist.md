---
layout: post
title: "VAE MNIST"
date: 2020-05-28
excerpt: "Create MNIST digits by Variational Autoencoder"
tags: [Project]
comments: true
---

## VAE MNIST

<img src="https://jaan.io/images/encoder-decoder.png" alt="img" style="zoom:50%;" />

---

### Vanilla VAE

- Loss function (Negative ELBO, NELBO)
  - $$l_i(\theta, \phi) = - \mathbb{E}_{z\sim q_\theta(z\mid x_i)}[\log p_\phi(x_i\mid z)] + \mathbb{KL}(q_\theta(z\mid x_i) \mid\mid p(z))$$
- Reconstructed Loss (Negative Log Likelihood, NLL)
  - $$- \mathbb{E}_{z\sim q_\theta(z\mid x_i)}[\log p_\phi(x_i\mid z)] = l_{i}(\tilde{x_i}, x_i)=-w_{i}\left[x_i \cdot \log \sigma\left(\tilde{x_i}\right)+\left(1-x_i\right) \cdot \log \left(1-\sigma\left(\tilde{x_i}\right)\right)\right]$$
  - $$w_{i} = 1$$
- KL Divergence
  - $$\mathbb{KL}(q_\theta(z\mid x_i) \mid\mid p(z)) = \frac{1}{2}(\log{\sigma_{p}^2} - \log{\sigma_{q}^2} + \frac{\sigma_{q}^2}{\sigma_{p}^2} + \frac{(\mu_{q} - \mu_{p})^2}{\sigma_{p}^2} - 1)$$
  - Assuming the approximate posterior, $$q_\theta(z\mid x_i) = \mathcal{N}{(\mu_{q}, \sigma_{q})}$$
  - Set prior as fixed parameter, $$p(z) = \mathcal{N}{(\mu_{p} = 0, \sigma_{p} = 1)}$$

---

### Gaussian Mixture VAE (GM-VAE) 

- Loss function (Negative ELBO, NELBO)
  - same as above
- Reconstructed Loss (Negative Log Likelihood, NLL)
  - same as above
- KL Divergence
  - In vanilla VAE, we simply assume that the prior, $$p(z)$$, is normally distributed. To get a better performance, in GM-VAE, we instead fit our posterior to $$p(z)$$, which is a mixture of gaussian.
  - $$\mathbb{KL}(q_\theta(z\mid x_i) \mid\mid p(z)) = \mathcal{LL}{(z, \mu_{q}, \sigma_{q})} - \sum_{j=1}^{k}{\pi_j \cdot \mathcal{LL}{(z, \mu_{p(j)}, \sigma_{p(j)})}}$$
  - $$\mathcal{LL}$$ means log likelihood
  - The second term is log likelihood of gaussian mixture where $$k$$ is the number of gaussian components in mixture. In the actual implementation, we simply set the mixture proportion $$\pi = \frac{1}{k}$$ for all $$k$$ mixture components.
  - For each gaussian component, $$\mathcal{N}{(\mu_{p(j)}, \sigma_{p(j)})}$$, $$\mu_{p(j)}$$ and $$\sigma_{p(j)}$$ are randomly sampled from a standard normal distribution, $$\mathcal{N}(0, 1)$$ and are scaled down properly.

---

### Experimental Results

#### Training Curves

###### VAE

![reconstructed loss of vae](https://user-images.githubusercontent.com/11435445/83091073-ee4cbb00-a0cc-11ea-8dff-98a752db51f8.png)
*REC loss of VAE*

![nelbo of vae](https://user-images.githubusercontent.com/11435445/83091075-f0af1500-a0cc-11ea-8316-71e974bbfcab.png)
*NELBO of VAE*

As shown above, simply increasing the size of $$z$$ is not always helpful on improving the performance of VAE. Instead, when using a relatively large $$z$$ (i.e 1000), it takes much more itertaions (~8k) for the model to reconstruct images well.

###### GMVAE

![reconstructed loss of gmvae](https://user-images.githubusercontent.com/11435445/83091086-f4429c00-a0cc-11ea-9ba1-a41ccfc28a27.png)
*REC loss of GMVAE*

![nelbo of gmvae](https://user-images.githubusercontent.com/11435445/83091090-f6a4f600-a0cc-11ea-92bd-2f94b66bae5e.png)
*NELBO of VAE*

#### Test Results

| N=20,000    | NELBO   | REC    |
| ----------- | ------- | ------ |
| VAE, z=10   | 98.735  | 79.388 |
| VAE, z=20   | 96.104  | 71.958 |
| VAE, z=50   | 95.947  | 70.720 |
| VAE, z=100  | 95.962  | 70.753 |
| VAE, z=1000 | 100.170 | 75.964 |

| N=20,000, z=20 | NELBO  | REC    |
| -------------- | ------ | ------ |
| GMVAE, k=1     | 95.159 | 71.105 |
| GMVAE, k=10    | 95.029 | 71.681 |
| GMVAE, k=100   | 93.431 | 70.782 |
| GMVAE, k=250   | 92.741 | 69.881 |
| GMVAE, k=500   | 92.397 | 69.786 |

#### Reconstructed and Generated Samples

###### Reconstructed

| VAE, z=20, N=20,000                                     | GMVAE, z=20, k=500, N=20,000                                 |
| ------------------------------------------------------- | ------------------------------------------------------------ |
| ![](https://user-images.githubusercontent.com/11435445/83091182-35d34700-a0cd-11ea-9783-fc7a085e494c.png) | ![](https://user-images.githubusercontent.com/11435445/83091191-38ce3780-a0cd-11ea-80fa-0544c3791837.png) |
| ![](https://user-images.githubusercontent.com/11435445/83091207-4388cc80-a0cd-11ea-8587-238adf8266d5.png) | ![](https://user-images.githubusercontent.com/11435445/83091218-47b4ea00-a0cd-11ea-88db-bfba13c64ce0.png) |

###### Generated

| VAE, z=20, N=20,000                                         | GMVAE, z=20, k=500, N=20,000                                 |
| ----------------------------------------------------------- | ------------------------------------------------------------ |
| ![](https://user-images.githubusercontent.com/11435445/83092507-0a059080-a0d0-11ea-81af-e8ac4f346a96.png) | ![](https://user-images.githubusercontent.com/11435445/83092517-0eca4480-a0d0-11ea-8b3a-dd15c76fcd79.png)  |
| ![](https://user-images.githubusercontent.com/11435445/83092534-17bb1600-a0d0-11ea-98cc-665251513c9e.png) | ![](https://user-images.githubusercontent.com/11435445/83092542-1db0f700-a0d0-11ea-8df6-66f7116a0267.png) |

###### GIFs

| ![](https://user-images.githubusercontent.com/11435445/83092639-489b4b00-a0d0-11ea-93f1-ea9340daee33.gif)                                        |
| ------------------------------------------------------- |
| Random generated samples from VAE (output w/ Bernoulli) |

| ![](https://user-images.githubusercontent.com/11435445/83092642-4a650e80-a0d0-11ea-9ba8-1c426883fcdf.gif)                                        |
| ------------------------------------------------------------ |
| Reconstructed samples from VAE given deterministic MNIST subset (N=20000, z=25) |

---

### Future Works

- using IWAE bound
- C-VAE
  - Semi-supervised and Full-supervised VAE
