---
layout: post
title: "【PR】SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"
date: 2020-06-12
excerpt: "Paper Reading"
tags: [Paper, Generative Model]
comments: true
---

**SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient**, Lantao Yu et al. 

[`Paper`](https://arxiv.org/abs/1609.05473)
[`Code`](https://github.com/suragnair/seqGAN)


## Sequence Generative Adversarial Nets

Train a $$\theta$$-parameterized generative model $$G_{\theta}$$ to produce a sequence, 

$$Y_{1: T}=\left(y_{1}, \ldots, y_{t}, \ldots, y_{T}\right), y_{t} \in \mathcal{Y}$$ (the vocabulary of candidate tokens)

Train a $$\phi$$-parameterized discriminative model $$D_{\phi}$$ to provide a guidance 

$$D_{\phi}(Y_{1:T})$$ - how likely $$Y_{1:T}$$ is from real data or not

From the perspective of RL, in timestep $$t$$,

***State*** - $$s = \left(y_{1}, \ldots, y_{t-1}\right)$$

***Action*** - $$a = y_{t}$$

***Policy Model*** - $$G_{\theta}\left(y_{t} \mid Y_{1: t-1}\right)$$

***State Transition*** is deterministic after an action has been chosen,  $$\delta_{s, s^{\prime}}^{a}=1$$ for the next state $$s^{\prime} = Y_{1:t}$$, if $$s = Y_{1:t-1}$$ and $$a = y_{t}$$

$$D_{\phi}$$ is trained by providing the real sequence data and the synthetic sequences generated from $$G_{\theta}$$

$$G_{\theta}$$ is updated by policy gradient and MC search on the expected end reward from $$D_{\phi}$$

The reward is estimated by $$D_{\phi}(Y_{1:T})$$

![](https://thu-coai.github.io/cotk_docs/_images/seqgan.png)
